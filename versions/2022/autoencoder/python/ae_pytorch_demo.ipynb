{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoEncoder PyTorch Demo using MNIST\n",
    "\n",
    "In this demo, we build a simple autoencoder using PyTorch. A separate encoder and decoder are built. The encoder is trained to encode the input data into a latent space. The decoder is trained to reconstruct the input data from the latent space.\n",
    "\n",
    "This demo also shows how to use an autoencoder to remove noise from images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rowel/anaconda3/envs/mspeech/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import time\n",
    "\n",
    "from torch import nn\n",
    "from einops import rearrange\n",
    "from argparse import ArgumentParser\n",
    "from lightning import LightningModule, Trainer\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Encoder using PyTorch\n",
    "\n",
    "We use 3 CNN layers to encode the input image. We use stride of 2 to reduce the feature map size. The last MLP layer resizes the flattened feature map to the target latent vector size, `feature_dim`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h.shape: torch.Size([1, 16])\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_features=1, kernel_size=3, n_filters=16, feature_dim=16):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(n_features, n_filters, kernel_size=kernel_size, stride=2)\n",
    "        self.conv2 = nn.Conv2d(n_filters, n_filters*2, kernel_size=kernel_size, stride=2)\n",
    "        self.conv3 = nn.Conv2d(n_filters*2, n_filters*4, kernel_size=kernel_size, stride=2)\n",
    "        self.fc1 = nn.Linear(256, feature_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = nn.ReLU()(self.conv1(x))\n",
    "        y = nn.ReLU()(self.conv2(y))\n",
    "        y = nn.ReLU()(self.conv3(y))\n",
    "        y = rearrange(y, 'b c h w -> b (c h w)')\n",
    "\n",
    "        y = self.fc1(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "# use this to get the correct input shape for  fc1. \n",
    "encoder = Encoder(n_features=1)\n",
    "x = torch.Tensor(1, 1, 28, 28)\n",
    "h = encoder(x)\n",
    "print(\"h.shape:\", h.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Decoder using PyTorch\n",
    "\n",
    "A decoder is used to reconstruct the input image. The decoder is trained to reconstruct the input data from the latent space. The architecture is similar to the encoder but inverted. A latent vector is resized using an MLP layer so that it is suitable for a convolutional layer. We use strided tranposed convolutional layers to upsample the feature map until the desired image size is reached. The last activation layer is a sigmoid to ensure the output is in the range [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_tilde.shape: torch.Size([1, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, kernel_size=3, n_filters=64, feature_dim=16, output_size=28, output_channels=1):\n",
    "        super().__init__()\n",
    "        self.init_size = output_size // 2**2 - 1\n",
    "        self.fc1 = nn.Linear(feature_dim, self.init_size**2 * n_filters)\n",
    "        # output size of conv2dtranspose is (h-1)*2 + 1 + (kernel_size - 1)\n",
    "        self.conv1 = nn.ConvTranspose2d(n_filters, n_filters//2, kernel_size=kernel_size, stride=2)\n",
    "        self.conv2 = nn.ConvTranspose2d(n_filters//2, n_filters//4, kernel_size=kernel_size, stride=2)\n",
    "        self.conv3 = nn.ConvTranspose2d(n_filters//4, output_channels, kernel_size=kernel_size-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, _ = x.shape\n",
    "        y = self.fc1(x)\n",
    "        y = rearrange(y, 'b (c h w) -> b c h w', b=B, h=self.init_size, w=self.init_size)\n",
    "        y = nn.ReLU()(self.conv1(y))\n",
    "        y = nn.ReLU()(self.conv2(y))\n",
    "        y = nn.Sigmoid()(self.conv3(y))\n",
    "\n",
    "        return y\n",
    "\n",
    "decoder = Decoder()\n",
    "x_tilde = decoder(h)\n",
    "print(\"x_tilde.shape:\", x_tilde.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch Lightning AutoEncoder\n",
    "\n",
    "An autoencoder is simply an encoder and a decoder. The encoder extracts the feature vector from the input image. The decoder reconstructs the input image from the feature vector. The feature vector is the compressed representation of the input image.\n",
    "\n",
    "Our PL module can also perform denoising. Below, we also present the collate function for clean and noisy images. To generate noisy images, we apply a Gaussian noise with mean of 0.5 and a standard deviation of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_collate_fn(batch):\n",
    "        x, _ = zip(*batch)\n",
    "        x = torch.stack(x, dim=0)\n",
    "        # mean=0.5, std=0.5 normal noise\n",
    "        noise = torch.normal(0.5, 0.5, size=x.shape)\n",
    "        xn = x + noise\n",
    "        xn = torch.clamp(xn, 0, 1)\n",
    "        return xn, x\n",
    "\n",
    "def clean_collate_fn(batch):\n",
    "        x, _ = zip(*batch)\n",
    "        x = torch.stack(x, dim=0)\n",
    "        return x, x\n",
    "\n",
    "class LitAEMNISTModel(LightningModule):\n",
    "    def __init__(self, feature_dim=16, lr=0.001, batch_size=64,\n",
    "                 num_workers=4, max_epochs=30, denoise=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.encoder = Encoder(feature_dim=feature_dim)\n",
    "        self.decoder = Decoder(feature_dim=feature_dim)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.denoise = denoise\n",
    "        self.losses = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        x_tilde = self.decoder(h)\n",
    "        return x_tilde\n",
    "\n",
    "    # this is called during fit()\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x_in, x = batch\n",
    "        x_tilde = self.forward(x_in)\n",
    "        loss = self.loss(x_tilde, x)\n",
    "        self.losses.append(loss.item())\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        avg_loss = torch.tensor(self.losses).mean()\n",
    "        self.log(\"train_loss\", avg_loss, on_epoch=True)\n",
    "        self.losses = []\n",
    "\n",
    "    # validation is the same as test\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "       return self.test_step(batch, batch_idx)\n",
    "\n",
    "    # we use Adam optimizer\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        # this decays the learning rate to 0 after max_epochs using cosine annealing\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=self.hparams.max_epochs)\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    # this is called after model instatiation to initiliaze the datasets and dataloaders\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataloader()\n",
    "        self.test_dataloader()\n",
    "\n",
    "    # build train and test dataloaders using MNIST dataset\n",
    "    # we use simple ToTensor transform\n",
    "    def train_dataloader(self):        \n",
    "        collate_fn = noise_collate_fn if self.denoise else clean_collate_fn\n",
    "        return torch.utils.data.DataLoader(\n",
    "            torchvision.datasets.MNIST(\n",
    "                \"./data\", train=True, download=True, \n",
    "                transform=torchvision.transforms.ToTensor()\n",
    "            ),\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        collate_fn = noise_collate_fn if self.denoise else clean_collate_fn\n",
    "        return torch.utils.data.DataLoader(\n",
    "            torchvision.datasets.MNIST(\n",
    "                \"./data\", train=False, download=True, \n",
    "                transform=torchvision.transforms.ToTensor()\n",
    "            ),\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.test_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arguments\n",
    "\n",
    "The arguments are as in our previous examples. The only new argument is `feature_dim`. This is the size of the latent vector. To use the denoising autoencoder, we also need to set `denoise` to `True`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = ArgumentParser(description=\"PyTorch Lightning AE MNIST Example\")\n",
    "    parser.add_argument(\"--max-epochs\", type=int, default=30, help=\"num epochs\")\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=64, help=\"batch size\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.001, help=\"learning rate\")\n",
    "\n",
    "    parser.add_argument(\"--feature-dim\", type=int, default=2, help=\"ae feature dimension\")\n",
    "    # if denoise is true\n",
    "    parser.add_argument(\"--denoise\", action=\"store_true\", help=\"train a denoising AE\")\n",
    "\n",
    "    parser.add_argument(\"--devices\", default=1)\n",
    "    parser.add_argument(\"--accelerator\", default='gpu')\n",
    "    parser.add_argument(\"--num-workers\", type=int, default=4, help=\"num workers\")\n",
    "    \n",
    "    args = parser.parse_args(\"\")\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training an AE\n",
    "\n",
    "We train the autoencoder on the MNIST dataset. For simple reconstruction, the input image is also the target image. For denoising, the input is the noisy image while the target is the clean image.\n",
    "\n",
    "The results can be viewed on [wandb](https://app.wandb.ai/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name    | Type    | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | encoder | Encoder | 23.8 K | train\n",
      "1 | decoder | Decoder | 30.1 K | train\n",
      "2 | loss    | MSELoss | 0      | train\n",
      "--------------------------------------------\n",
      "53.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "53.9 K    Total params\n",
      "0.215     Total estimated model params size (MB)\n",
      "11        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 938/938 [00:04<00:00, 201.95it/s, v_num=8]       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 938/938 [00:04<00:00, 201.44it/s, v_num=8]\n",
      "Elapsed time: 139.42752122879028\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args = get_args()\n",
    "    ae = LitAEMNISTModel(feature_dim=args.feature_dim, lr=args.lr, \n",
    "                         batch_size=args.batch_size, num_workers=args.num_workers,\n",
    "                         denoise=args.denoise, max_epochs=args.max_epochs)\n",
    "    ae.setup()\n",
    "    start_time = time.time()\n",
    "    trainer = Trainer(accelerator=args.accelerator,\n",
    "                      devices=args.devices,\n",
    "                      strategy=\"auto\",\n",
    "                      max_epochs=args.max_epochs,)\n",
    "    trainer.fit(ae)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"Elapsed time: {}\".format(elapsed_time))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_in.shape: torch.Size([1, 2])\n",
      "x_in: tensor([[0.3865, 0.3721]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    # decoder as a generative model\n",
    "    import matplotlib.pyplot as plt\n",
    "    decoder = ae.decoder\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        # generate a tensor of random noise with size 1, feature_dim\n",
    "        x_in = torch.randn(1, args.feature_dim)\n",
    "        print(\"x_in.shape:\", x_in.shape)\n",
    "        print(\"x_in:\", x_in)\n",
    "        x_tilde = decoder.forward(x_in)\n",
    "        plt.imshow(x_tilde[0].detach().numpy().reshape(28, 28), cmap=\"gray\")\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mspeech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
